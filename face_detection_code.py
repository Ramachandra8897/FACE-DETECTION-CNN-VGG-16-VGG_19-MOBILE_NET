# -*- coding: utf-8 -*-
"""face_detection_code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13SV63MTiNGND7a74kqMZ6I_Ll2IBTW2W
"""

# Commented out IPython magic to ensure Python compatibility.
#list of useful imports that  I will use
# %matplotlib inline
import os
import tqdm
import matplotlib.pyplot as plt
import pandas as pd
import cv2
import numpy as np
from glob import glob
import seaborn as sns
import random
from keras.preprocessing import image
import tensorflow as tf

#from keras.utils.np_utils import to_categorical # convert to one-hot-encoding
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, GlobalMaxPooling2D
#from keras.optimizers import RMSprop
#from keras.preprocessing.image import ImageDataGenerator
#from keras.optimizers import Adam
from sklearn.model_selection import train_test_split

# Run this cell to mount your Google Drive.
from google.colab import drive
drive.mount('/content/drive')

file = '/content/drive/MyDrive/face_emotion.zip'

import zipfile as zf
data_zip = zf.ZipFile(file)
data_zip.extractall()
!ls

data = 'CK+48'

#in this we import all the images to the Images.
#in this we append img to images

Images = []
import os
for dirname, _, filenames in os.walk(data):
    for filename in filenames:
        img = os.path.join(dirname, filename)
        Images.append(img)

Images[:10]

len(Images)

#in this we append class label names only
#anger only is append

Class_label = []
for i in Images:
  j = i.split("/")
  #print(j[1])
  Class_label.append(j[1])

Class_label[:10]

len(Class_label)

# Shuffle two lists with same order
# Using zip() + * operator + shuffle()
#by using this we combine both by using the zip
#[img1,cat],[img2,dog]by using this we come like this
temp = list(zip(Images, Class_label))
random.shuffle(temp)
Images, Class_label = zip(*temp)

data = pd.DataFrame(list(zip(Images, Class_label)), columns =['Image_path', 'Class_label'])

data.shape

data.head(10)

data.Class_label.value_counts()

from sklearn.utils import resample
#seperate majority and minority classses
df_c0=data[data["Class_label"]=="surprise"]
df_c1=data[data["Class_label"]=="happy"]
df_c3=data[data["Class_label"]=="anger"]
df_c4=data[data["Class_label"]=="sadness"]
df_c5=data[data["Class_label"]=="fear"]       #we take top 3 as majority class and remove the one least one from them

#top 3 majority class has 249,207,177,we remove 177
#top 4 minority class has 135,84,75,54 we remove 54


#upsample majority class
df_c0_upsampled=resample(df_c0,replace=True,n_samples=500,random_state=123)#we convert samples into the 500 each one
df_c1_upsampled=resample(df_c1,replace=True,n_samples=500,random_state=123)
df_c3_upsampled=resample(df_c3,replace=True,n_samples=500,random_state=123)
df_c4_upsampled=resample(df_c4,replace=True,n_samples=500,random_state=123)
df_c5_upsampled=resample(df_c5,replace=True,n_samples=500,random_state=123)


#combine minority class with downsampled majority class
df_upsampled=pd.concat([df_c0_upsampled,df_c1_upsampled,df_c3_upsampled,df_c4_upsampled,df_c5_upsampled])

#Display new class counts

df_upsampled["Class_label"].value_counts()

#counts of top 10 drugs
sns.set(style="whitegrid")
plt.figure(figsize=(10,5))
ax=sns.countplot(x="Class_label",data=df_upsampled,palette=sns.color_palette("cubehelix",4))
plt.xticks(rotation=90)
plt.title("Class Label Counts", {"fontname":"fantasy", "fontweight":"bold", "fontsize":"medium"})
plt.ylabel("count", {"fontname": "serif", "fontweight":"bold"})
plt.xlabel("Class_label", {"fontname": "serif", "fontweight":"bold"})

#shuffle the data or mixed the data frac=1 is used for that
data=df_upsampled.sample(frac=1)

data.head()

"""# **RESIZE THE IMAGE**"""

def resize_images(img):
  #file=image.open(img)
  file=cv2.imread(img)
  #img=file.convert("RGB")
  #img_bgr=img.resize((48,48))
  resized=cv2.resize(file,(48,48),interpolation=cv2.INTER_AREA)#here by using this we convert all images in to 48,48 size only
  img_bgr=np.array(resized)
  return img_bgr

from PIL import Image

images=[resize_images(img)for img in data["Image_path"]]

len(images)

images[0]

#print number of classes in our dataset
num_classes=len(np.unique(data["Class_label"]))

num_classes

#save the class into class_names
class_names=list(data["Class_label"])

images[0]

#see the image with class label
plt.imshow(images[5])
plt.title(class_names[5])

#See the image with class label
plt.imshow(images[10])
plt.title(class_names[10])

#See the image with class label
plt.imshow(images[400])
plt.title(class_names[400])

#See the image with class label
plt.imshow(images[70])
plt.title(class_names[70])

"""# **LABEL ENCODER**"""

from sklearn.preprocessing import LabelBinarizer
enc=LabelBinarizer()
y=enc.fit_transform(data["Class_label"])
#by using this we convert to label encoder

data["Class_label"][:10]

images=np.array(images)#CONVERT INTO THE ARRAY

images.shape

y.shape

y

"""# **SPLITING DATA INTO TRAINING AND TESTING**"""

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(images,y,test_size=0.3,stratify=y,random_state=42)

x_train.shape

x_test.shape

"""# **cnn  model**"""

#set the Cnn model
batch_size=None
model=Sequential()

# First Convolutional Layer:
# - Filters: 32 filters are used to detect features like edges, textures, etc.
# - Kernel size: Each filter is 5x5 pixels.
# - Padding: "Same" padding ensures the output size is the same as input size.
# - Activation function: ReLU is used to add non-linearity.
# - Input shape: The images have a size of 48x48 pixels and 3 channels (RGB).


model.add(Conv2D(filters=32,kernel_size=(5,5),padding="Same",activation="relu",input_shape=(48,48,3)))

# Second Convolutional Layer:
# - Another 32 filters with 5x5 kernel size.
# - ReLU activation for non-linearity.
model.add(Conv2D(filters=32,kernel_size=(5,5),padding="Same",activation="relu"))

# Max Pooling Layer:
# - Pool size: (2,2), which reduces the size of the image by taking the maximum value in a 2x2 block.
model.add(MaxPool2D(pool_size=(2,2)))

# Dropout Layer:
# - Dropout of 20% of the neurons to help prevent overfitting.
model.add(Dropout(0.2))
# Third Convolutional Layer:
# - Filters increased to 64 to learn more complex features.
# - 3x3 kernel size, with "Same" padding and ReLU activation.
model.add(Conv2D(filters=64,kernel_size=(3,3),padding="Same",activation="relu"))

# Fourth Convolutional Layer:
# - Another 64 filters with the same configuration.
model.add(Conv2D(filters=64,kernel_size=(3,3),padding="Same",activation="relu"))

# Max Pooling Layer (with stride):
# - Reduces the image size again, with a stride of 2, reducing the spatial dimensions.
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
# Dropout Layer:
# - Dropout of 30% of neurons to prevent overfitting further.
model.add(Dropout(0.3))


# Fifth Convolutional Layer:
# - Filters increased again to 128 to learn even more abstract features.
# - 3x3 kernel size with "Same" padding and ReLU activation.
model.add(Conv2D(filters=128,kernel_size=(3,3),padding="Same",activation="relu"))

# Sixth Convolutional Layer:
# - Another 128 filters for further feature extraction.
model.add(Conv2D(filters=128,kernel_size=(3,3),padding="Same",activation="relu"))


# Max Pooling Layer (with stride):
# - Another max pooling layer to reduce the spatial size.
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

# Dropout Layer:
# - Dropout of 40% of neurons for further regularization.
model.add(Dropout(0.4))

# Global Max Pooling Layer:
# - This layer reduces the entire feature map into a single value (max value from the entire feature map).
# - It helps reduce dimensionality and prevent overfitting by selecting the most significant feature.
model.add(GlobalMaxPooling2D())

# Fully Connected (Dense) Layer:
# - 256 neurons, with ReLU activation.
# - This layer interprets the features extracted by previous layers.
model.add(Dense(256,activation="relu"))

# Dropout Layer:
# - Dropout of 50% to avoid overfitting and improve generalization.
model.add(Dropout(0.5))

# Output Layer:
# - This is the final layer, with 5 neurons representing 5 different classes.
# - Softmax activation is used to output probabilities for each of the 5 classes, with the sum of probabilities equal to 1.
model.add(Dense(5,activation="softmax"))


# Display the model summary (number of layers, parameters, etc.)
model.summary()

model.compile(optimizer="Adam",loss="categorical_crossentropy",metrics=["accuracy"])
# Optimizer: Adam (adaptive optimization algorithm), # Loss Function: Categorical Crossentropy (for multi-class classification), # Metrics: Accuracy (percentage of correct predictions)

import warnings
warnings.filterwarnings("ignore")#it remove the warnings

#fit the model
history=model.fit(x_train,y_train,epochs=20,validation_data=(x_test,y_test),batch_size=128) #here we take the epochs

#plot the accuracy plot
plt.plot(history.history["accuracy"], "r")
plt.plot(history.history["val_accuracy"],"b")
plt.legend({"Train Accuracy":"r","Test Accuracy":"b"})
plt.show()

#plot the accuracy plot
plt.plot(history.history["loss"],"r")
plt.plot(history.history["val_loss"],"b")
plt.legend({"Train Loss":"r","Test Loss":"b"})
plt.show()

#print the test accuracy
score_1=model.evaluate(x_test,y_test,verbose=0)
print("Test accuracy:",score_1[1])

results=pd.DataFrame(columns=["Model","Test Accuracy"])

new=["CNN",score_1[1]]
results.loc[1]=new

#plot confusion matrix
from sklearn.metrics import confusion_matrix
class_names=enc.classes_
df_heatmap=pd.DataFrame(confusion_matrix(np.argmax(model.predict(x_test),axis=1),np.argmax(y_test,axis=1)),columns=class_names,index=class_names)
heatmap=sns.heatmap(df_heatmap,annot=True,fmt="d")

df_heatmap

#print images with actual and predicted class labels
# print images with actiual abnd predicted class labels
for i in range(20):
  plt.figure(figsize=(15,15))
  plt.subplot(4,5,i+1)
  pred = model.predict(np.array([x_test[i]]))[0]
  pred = np.argmax(pred)
  act = np.argmax(y_test[i])
  plt.title("Predicted class: {}\n Actual class: {}".format(enc.classes_[pred],enc.classes_[act]))
  # plt.title("Actual class: {}".format(enc.classes_[act]))
  plt.imshow(x_test[i])



"""# **VGG_16 MODEL**"""

from keras.applications.vgg16 import VGG16
vgg=VGG16(weights="imagenet",include_top=False,input_shape=(48,48,3))
vgg.trainable=False

#set the vgg16 model
model_1=Sequential()
model_1.add(vgg)
model_1.add(Flatten())
model_1.add(Dense(128,activation="relu"))
model_1.add(Dropout(0.2))
model_1.add(Dense(5,activation="softmax"))

#compile the model
model_1.compile(optimizer="Adam",loss="categorical_crossentropy",metrics=["accuracy"])

History_1=model_1.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test),batch_size=128)

# print the test accuracy
score_2 = model_1.evaluate(x_test, y_test, verbose=0)
print('Test accuracy:', score_2[1])

new = ['VGG-16 ',score_2[1]]
results.loc[2] = new

#plot confusion matrix
from sklearn.metrics import confusion_matrix
class_names = enc.classes_
df_heatmap = pd.DataFrame(confusion_matrix(np.argmax(model_1.predict(x_test),axis = 1),np.argmax(y_test,axis=1)),columns=class_names, index = class_names)
heatmap = sns.heatmap(df_heatmap, annot=True, fmt="d")

# plot the accuracy plot
plt.plot(History_1.history['accuracy'], 'r')
plt.plot(History_1.history['val_accuracy'], 'b')
plt.legend({'Train Accuracy': 'r', 'Test Accuracy':'b'})
plt.show()

# plot the accuracy plot
plt.plot(History_1.history['loss'], 'r')
plt.plot(History_1.history['val_loss'], 'b')
plt.legend({'Train Loss': 'r', 'Test Loss':'b'})
plt.show()

# print images with actiual abnd predicted class labels
for i in range(20):
  plt.figure(figsize=(15,15))
  plt.subplot(4,5,i+1)
  pred = model_1.predict(np.array([x_test[i]]))[0]
  pred = np.argmax(pred)
  act = np.argmax(y_test[i])
  plt.title("Predicted class: {}\n Actual class: {}".format(enc.classes_[pred],enc.classes_[act]))
  # plt.title("Actual class: {}".format(enc.classes_[act]))
  plt.imshow(x_test[i])

"""## ***VGG_19 MODEL***"""

# import the vgg16 model
from keras.applications.vgg19 import VGG19

vgg=VGG19(weights='imagenet',include_top=False,input_shape=(48,48,3))

#Fit the data or train the model
History_1 = model_1.fit(x_train, y_train, epochs = 20, validation_data = (x_test,y_test),batch_size = 128)

# print the test accuracy
score_2 = model_1.evaluate(x_test, y_test, verbose=0)
print('Test accuracy:', score_2[1])

new = ['VGG-19 ',score_2[1]]
results.loc[3] = new

#plot confusion matrix
from sklearn.metrics import confusion_matrix
class_names = enc.classes_
df_heatmap = pd.DataFrame(confusion_matrix(np.argmax(model_1.predict(x_test),axis = 1),np.argmax(y_test,axis=1)),columns=class_names, index = class_names)
heatmap = sns.heatmap(df_heatmap, annot=True, fmt="d")

# plot the accuracy plot
plt.plot(History_1.history['accuracy'], 'r')
plt.plot(History_1.history['val_accuracy'], 'b')
plt.legend({'Train Accuracy': 'r', 'Test Accuracy':'b'})
plt.show()

# plot the accuracy plot
plt.plot(History_1.history['loss'], 'r')
plt.plot(History_1.history['val_loss'], 'b')
plt.legend({'Train Loss': 'r', 'Test Loss':'b'})
plt.show()

# print images with actiual abnd predicted class labels
for i in range(20):
  plt.figure(figsize=(15,15))
  plt.subplot(4,5,i+1)
  pred = model_1.predict(np.array([x_test[i]]))[0]
  pred = np.argmax(pred)
  act = np.argmax(y_test[i])
  plt.title("Predicted class: {}\n Actual class: {}".format(enc.classes_[pred],enc.classes_[act]))
  # plt.title("Actual class: {}".format(enc.classes_[act]))
  plt.imshow(x_test[i])

"""# **MOBILE NET**"""

# import the vgg16 model
from keras.applications.mobilenet import MobileNet
#from tf.keras.applications.mobilenet.MobileNet

mob=MobileNet(weights='imagenet',include_top=False,input_shape=(48,48,3))

mob.trainable=False

# Set the vgg16 model

model_1=Sequential()
model_1.add(mob)
model_1.add(Flatten())
model_1.add(Dense(128, activation='relu'))
model_1.add(Dropout(0.2))
model_1.add(Dense(5, activation='softmax'))

#Compile the model
model_1.compile(optimizer = "Adam", loss = "categorical_crossentropy", metrics = ["accuracy"])

from tensorflow.keras.utils import plot_model # Import plot_model from the correct location
plot_model(model_1, to_file='model_plot.png', show_shapes=False, show_layer_names=True)

#Fit the data or train the model
History_1 = model_1.fit(x_train, y_train, epochs = 20, validation_data = (x_test,y_test),batch_size = 128)

# print the test accuracy
score_2 = model_1.evaluate(x_test, y_test, verbose=0)
print('Test accuracy:', score_2[1])

new = ['Mobile Net ',score_2[1]]
results.loc[4] = new

#plot confusion matrix
from sklearn.metrics import confusion_matrix
class_names = enc.classes_
df_heatmap = pd.DataFrame(confusion_matrix(np.argmax(model_1.predict(x_test),axis = 1),np.argmax(y_test,axis=1)),columns=class_names, index = class_names)
heatmap = sns.heatmap(df_heatmap, annot=True, fmt="d")

# plot the accuracy plot
plt.plot(History_1.history['accuracy'], 'r')
plt.plot(History_1.history['val_accuracy'], 'b')
plt.legend({'Train Accuracy': 'r', 'Test Accuracy':'b'})
plt.show()



# plot the accuracy plot
plt.plot(History_1.history['loss'], 'r')
plt.plot(History_1.history['val_loss'], 'b')
plt.legend({'Train Loss': 'r', 'Test Loss':'b'})
plt.show()

# print images with actiual abnd predicted class labels
for i in range(20):
  plt.figure(figsize=(15,15))
  plt.subplot(4,5,i+1)
  pred = model_1.predict(np.array([x_test[i]]))[0]
  pred = np.argmax(pred)
  act = np.argmax(y_test[i])
  plt.title("Predicted class: {}\n Actual class: {}".format(enc.classes_[pred],enc.classes_[act]))
  # plt.title("Actual class: {}".format(enc.classes_[act]))
  plt.imshow(x_test[i])

"""# *PERFORMANCE TABLE*"""

results